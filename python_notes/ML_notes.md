# ML Notes

## 1- PopÃ¼lasyon Standart Sapma ve Ã–rneklem Standart Sapma

### 1) PopÃ¼lasyon Standart Sapma ($\sigma$)
---
- **TanÄ±m:** Bir **bÃ¼tÃ¼n kitle (popÃ¼lasyon)** iÃ§erisindeki tÃ¼m verilerin ortalamadan ne kadar saptÄ±ÄŸÄ±nÄ± Ã¶lÃ§er.  
- Elimizde **tÃ¼m veriler** vardÄ±r (Ã¶rneÄŸin bir ÅŸehirde yaÅŸayan herkesin boyu).  
- **FormÃ¼l:**

$$
\sigma = \sqrt{\frac{\sum_{i=1}^{N} (x_i - \mu)^2}{N}}
$$

- AÃ§Ä±klama:
  - $x_i$ â†’ her bir veri  
  - $\mu$ â†’ popÃ¼lasyon ortalamasÄ±  
  - $N$ â†’ toplam veri sayÄ±sÄ±  

> Burada **bÃ¶lÃ¼m N**â€™dir, Ã§Ã¼nkÃ¼ tÃ¼m popÃ¼lasyonu biliyoruz.
---
```python
import numpy as np

speed = [48,49,50,51,52]

print(f"PopÃ¼lasyon Standart Sapma: {np.std(speed,ddof=0)}") # ddof = 0 iken PopÃ¼lasyon Standart Sapma bulunur. 
```
### 2) Ã–rneklem Standart Sapma ($s$)
---
- **TanÄ±m:** PopÃ¼lasyondan alÄ±nmÄ±ÅŸ bir **Ã¶rneklem** (kÃ¼Ã§Ã¼k grup) verilerinin ortalamadan sapmasÄ±nÄ± Ã¶lÃ§er.  
- GerÃ§ek popÃ¼lasyonu bilmediÄŸimiz iÃ§in sadece Ã¶rneklemden tahmin yaparÄ±z.  
- **FormÃ¼l:**

$$
s = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}}
$$

- AÃ§Ä±klama:
  - $x_i$ â†’ her bir Ã¶rneklem verisi  
  - $\bar{x}$ â†’ Ã¶rneklem ortalamasÄ±  
  - $n$ â†’ Ã¶rneklemdeki veri sayÄ±sÄ±  

> Burada **bÃ¶lÃ¼m n-1**â€™dir. Buna **Bessel dÃ¼zeltmesi** denir. AmaÃ§, popÃ¼lasyonun standart sapmasÄ±nÄ± daha doÄŸru tahmin edebilmektir.
---
```python
import numpy as np

speed = [48,49,50,51,52]

print(f"Ã–rneklem Standart Sapma: {np.std(speed,ddof=1)}") # ddof = 1 iken Ã–rneklem Standart Sapma bulunur.
```
### 3) Ã–zetle Fark
---
| Ã–zellik | PopÃ¼lasyon Std. Sapma ($\sigma$) | Ã–rneklem Std. Sapma ($s$) |
|---------|---------------------------|-------------------------|
| Veri kÃ¼mesi | TÃ¼m popÃ¼lasyon | Ã–rneklem |
| BÃ¶lÃ¼m | $N$ | $n-1$ |
| KullanÄ±m | GerÃ§ek daÄŸÄ±lÄ±mÄ± Ã¶lÃ§mek | PopÃ¼lasyonu tahmin etmek |
| Hata dÃ¼zeltme | Yok | Var (Bessel dÃ¼zeltmesi) |

## 2- Varyans

**Varyans**, bir veri kÃ¼mesindeki deÄŸerlerin **ortalama etrafÄ±nda ne kadar daÄŸÄ±ldÄ±ÄŸÄ±nÄ±** Ã¶lÃ§en istatistiksel bir kavramdÄ±r.  

- Standart sapmanÄ±n karesidir.
- EÄŸer bÃ¼tÃ¼n deÄŸerler ortalamaya Ã§ok yakÄ±nsa, varyans **kÃ¼Ã§Ã¼k** Ã§Ä±kar.  
- EÄŸer deÄŸerler ortalamadan Ã§ok uzaksa, varyans **bÃ¼yÃ¼k** Ã§Ä±kar.  
- **Yorum yapmak** iÃ§in standart sapma, **hesaplama ve teori** iÃ§in varyans kullanÄ±lÄ±r.
---

### *Matematiksel TanÄ±m :*
---
Bir veri kÃ¼mesinde deÄŸerler:  

$$
x_1, x_2, ..., x_n
$$

Ortalama (aritmetik ortalama):  

$$
\mu = \frac{1}{N} \sum_{i=1}^{N} x_i
$$

Varyans:  

$$
\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2
$$

Standart sapma:  

$$
\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2}
$$
---
```python
import numpy as np

speed = [48,49,50,51,52]

print(f"PopÃ¼lasyon VaryansÄ±: {np.var(speed,ddof=0)}")
print(f"Ã–rneklem VaryansÄ±: {np.var(speed,ddof=1)}")
```
## 3- Percentiles
ğŸ“Œ Percentile Nedir?

Percentile, verilerin sÄ±ralÄ± daÄŸÄ±lÄ±mÄ± iÃ§inde bir deÄŸerin konumunu belirtir.

Ã–rneÄŸin:

- 25. percentile (Q1): Verilerin %25â€™i bu deÄŸerin altÄ±nda yer alÄ±r.
- 50. percentile (median): Verilerin %50â€™si bu deÄŸerin altÄ±nda, %50â€™si Ã¼stÃ¼nde.
- 90. percentile: Verilerin %90â€™Ä± bu deÄŸerin altÄ±nda.

MLâ€™de genellikle:

- Feature scaling / normalization (Ã¶zellik Ã¶lÃ§ekleme),
- Outlier detection (aykÄ±rÄ± deÄŸer tespiti),
- Model deÄŸerlendirme (Ã¶rn. latency %95 percentile)
gibi yerlerde kullanÄ±lÄ±r.

```python
import numpy as np

notes = [1,2,5,7,8,10,12,15,18,20]

print(f"25. percentile: ", np.percentile(notes,25)) 
print(f"50. percentile: ", np.percentile(notes,50))
print(f"75. percentile: ", np.percentile(notes,75))
print(f"90. percentile: ", np.percentile(notes,90))
```
**Ã§Ä±ktÄ± :**
```python
25. percentile: 5.75
50. percentile: 9.0
75. percentile: 14.25
90. percentile: 18.2
```
- **Ã–reÄŸin ilk Ã§Ä±ktÄ± sayÄ±larÄ±n %25'inin 5.75 veya ondan daha az bir deÄŸere sahip olduÄŸunu gÃ¶steriyor.**

### ML'de KullanÄ±m Ã–rneÄŸi : 
- Outlier detection (aykÄ±rÄ± deÄŸerleri temizleme):
```python
import numpy as np

# Ã–rnek veri (iÃ§inde aykÄ±rÄ± deÄŸer var)
data = np.array([10, 12, 14, 15, 16, 18, 20, 100])

# 5. ve 95. yÃ¼zdelikleri al
low,high = np.percentile(data, [5,95])

# Bu aralÄ±ÄŸÄ±n dÄ±ÅŸÄ±nda kalanlarÄ± "outlier" say
filtered_data = data[(data>=low) & (data<= high)]

print("Orijinal veri:", data)
print("FiltrelenmiÅŸ veri:", filtered_data)
```
**Ã§Ä±ktÄ± :**
```python
Orijinal veri: [ 10  12  14  15  16  18  20 100]
FiltrelenmiÅŸ veri: [10 12 14 15 16 18 20]
```
> Burada 100, aykÄ±rÄ± deÄŸer olarak Ã§Ä±karÄ±ldÄ±.

## 4- Ã–lÃ§eklendirme (Scaling)
Verilerin belirli bir aralÄ±ÄŸa dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi veya standardize edilmesi iÅŸlemidir.Ã‡Ã¼nkÃ¼ farklÄ± Ã¶lÃ§eklerdeki veriler algoritmalarÄ±n performansÄ±nÄ± olumsuz etkileyebilir.

### 1. Min-Max Scaling (Normalizasyon)
- Verileri 0 ile 1 arasÄ±na (ya da belirlenen baÅŸka bir aralÄ±ÄŸa) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

**FormÃ¼l :**
$$
X' = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
$$
- Avantaj: Ã–zellikle nÃ¶ral aÄŸlar gibi algoritmalarda yaygÄ±n.

- Dezavantaj: AykÄ±rÄ± deÄŸerler (outlier) varsa Ã§ok etkilenir.
- Xâ€², "dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ" ya da "Ã¶lÃ§eklendirilmiÅŸ" yeni deÄŸerleri ifade etmek iÃ§in kullanÄ±lan bir gÃ¶sterimdir.

**Ã–rnek :**
```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

notes = np.array([[50],[60],[70],[80],[90]])

scaler = MinMaxScaler()

result = scaler.fit_transform(notes)

print(result)
```
**Ã‡Ä±ktÄ± :** 
```python
[[0.  ]
 [0.25]
 [0.5 ]
 [0.75]
 [1.  ]]
```
### 2. Standardizasyon (Z-score Scaling)
- Verilerin ortalamasÄ±nÄ± 0, standart sapmasÄ±nÄ± 1 yapar.

**FormÃ¼l :**
$$
X' = \frac{X - \mu}{\sigma}
$$
- Avantaj: AykÄ±rÄ± deÄŸerlerden Min-Max kadar etkilenmez.
- KullanÄ±m: Lineer regresyon, lojistik regresyon, SVM, PCA gibi yÃ¶ntemlerde Ã§ok yaygÄ±n.

**Ã–rnek :**
```python
from sklearn.preprocessing import StandardScaler
import pandas as pd

data = {
    "Height":[160,170,180,190],
    "Weight":[60,70,80,90]
}

df = pd.DataFrame(data)

scaler = StandardScaler()

result = scaler.fit_transform(df)

print(pd.DataFrame(result,columns=["Height","Weight"]))
```
**Ã‡Ä±ktÄ± :**
```python
     Height    Weight
0 -1.341641 -1.341641
1 -0.447214 -0.447214
2  0.447214  0.447214
3  1.341641  1.341641
```
**Ã–rnek kullanÄ±m :**
```python
from sklearn.preprocessing import StandardScaler
from sklearn import linear_model
import pandas as pd

# Standard Ã¶lÃ§eklendirmenin nesnesini oluÅŸturduk
scale = StandardScaler()

# Veri okuma ve x,y deÄŸiÅŸkenlerine deÄŸerler atanÄ±yor. X girdiler, y ise istenilen deÄŸerdir
df = pd.read_csv("data (1).csv")
X = df[["Weight","Volume"]]
y = df["CO2"]

# X deÄŸerlerini Ã¶lÃ§eklendirdik
scaledX = scale.fit_transform(X)

# Ã–ÄŸrenmeyi Ã¶lÃ§eklendirilmiÅŸ deÄŸerler Ã¼zerinden yapar
regr = linear_model.LinearRegression()
regr.fit(scaledX,y)

# Tahmin edilmesi istenen veri girildi ve Ã¶lÃ§eklendiridi
new_data = pd.DataFrame([[2300,1.3]],columns=["Weight","Volume"])
scaled = scale.transform(new_data)

# Tahmin yap
result = regr.predict(scaled)
print(result)
```
Ã‡Ä±ktÄ± : 
```python
[107.2087328]
```

## 5- Train-test yapÄ±sÄ±
Makine Ã¶ÄŸrenmesinde elimizde genelde etiketli bir veri seti olur (Ã¶rneÄŸin Ã¶ÄŸrenci notlarÄ± ve baÅŸarÄ± durumu, ev fiyatlarÄ± ve Ã¶zellikleri vs.).
- Bu veri setini ikiye ayÄ±rÄ±rÄ±z:

  - Train (eÄŸitim verisi) â†’ Modelin Ã¶ÄŸrenmesi iÃ§in kullandÄ±ÄŸÄ±mÄ±z kÄ±sÄ±m.

  - Test (test verisi) â†’ Modelin daha Ã¶nce gÃ¶rmediÄŸi verilerle sÄ±nanmasÄ± iÃ§in kullandÄ±ÄŸÄ±mÄ±z kÄ±sÄ±m.

**AmaÃ§ :** Model sadece ezber yapmasÄ±n, genelleme yapabilsin.

**Ã–rnek :**
```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import pandas as pd

df = pd.read_csv("data (1).csv")
X = df[["Weight","Volume"]]
y = df["CO2"]

# Veriyi eÄŸitim ve test olarak ayÄ±rma
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)

# Model oluÅŸturma ve eÄŸitme
regr = LinearRegression()
regr.fit(X_train,y_train)

# RÂ² skorunu hesaplama ve yazdÄ±rma
y_pred = regr.predict(X_test) 
score = r2_score(y_test,y_pred)
print("r2 score: ",score)

# GerÃ§ek ve tahmini deÄŸerleri yan yana gÃ¶stermek
new_df = pd.DataFrame({"GerÃ§ek CO2":y_test,"Tahmini CO2":y_pred})
print(new_df)
```
**Ã‡Ä±ktÄ± :**
```python
r2 score:  0.4100871476339105

    GerÃ§ek CO2  Tahmini CO2
35         120   106.690781
13          94   101.322166
26         104   104.965287
30         115   106.276007
16          99   102.136459
31         117   106.810057
21          99   104.518507
12          99    97.421216
8           98    99.831293
17         104   104.416030
9           99   100.587141
```
> Tahminler kÃ¶tÃ¼ Ã§Ã¼nkÃ¼ veri az.

## 6- KarÄ±ÅŸÄ±klÄ±k Matrisi (Confusion Matrix)
KarÄ±ÅŸÄ±klÄ±k matrisi, sÄ±nÄ±flandÄ±rma modellerinin tahminleri ile gerÃ§ek etiketler arasÄ±ndaki karÅŸÄ±laÅŸtÄ±rmayÄ± hÃ¼cresel olarak gÃ¶steren bir tablodur. Her satÄ±r gerÃ§ek sÄ±nÄ±fÄ±, her sÃ¼tun modelin tahmin ettiÄŸi sÄ±nÄ±fÄ± gÃ¶sterir. Bu sayede modelin hangi sÄ±nÄ±flarÄ± karÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±, hangi sÄ±nÄ±flarda iyi veya kÃ¶tÃ¼ olduÄŸunu rahatÃ§a gÃ¶rebilirsiniz.

**Binom (binary) durumda hÃ¼crelerin yorumu**

Tipik dÃ¼zen (scikit-learn confusion_matrix Ã§Ä±ktÄ±sÄ±):

     [[TN, FP],
      [FN, TP]]
- TN (True Negative): GerÃ§ek negatif ve tahmin negatif
- FP (False Positive): GerÃ§ek negatif ama tahmin pozitif (yanlÄ±ÅŸ alarm)
- FN (False Negative): GerÃ§ek pozitif ama tahmin negatif (kaÃ§Ä±rma)
- TP (True Positive): GerÃ§ek pozitif ve tahmin pozitif

**Bu hÃ¼crelerden ÅŸu metrikler hesaplanÄ±r :**

- Accuracy = (TP+TN) / toplam
- Precision = TP / (TP+FP) â€” pozitif tahminlerin doÄŸruluÄŸu
- Recall (Sensitivity) = TP / (TP+FN) â€” gerÃ§ek pozitiflerin yakalanma oranÄ±
- F1 = 2 * (Precision * Recall) / (Precision + Recall)
```python
from sklearn import metrics
import numpy as np
import matplotlib.pyplot as plt

rng = np.random.default_rng()

actual = rng.binomial(1,0.9,size=1000) # data
predicted = rng.binomial(1,0.9,size=1000) # model tahmini Ã¶rneÄŸi

Accuary = metrics.accuracy_score(actual,predicted)

Precision = metrics.precision_score(actual,predicted)

Sensitivity = metrics.recall_score(actual,predicted)

Specificity = metrics.recall_score(actual,predicted,pos_label=0)

F1_score = metrics.f1_score(actual,predicted)

print("Accuary: ",Accuary)
print("Precision: ",Precision)
print("Sensitivity: ",Sensitivity)
print("Specificity: ",Specificity)
print("F1_score: ",F1_score)
```

## 7- Grid Search
Grid Search (Ä±zgara arama), bir modelin performansÄ±nÄ± artÄ±rmak iÃ§in hiperparametrelerin en iyi kombinasyonunu bulmayÄ± amaÃ§lar.
- Hiperparametre: Model eÄŸitimi sÄ±rasÄ±nda kullanÄ±cÄ± tarafÄ±ndan belirlenen deÄŸerler (Ã¶r. C veya kernel SVMâ€™de).

- Grid Search, Ã¶nceden belirlenmiÅŸ tÃ¼m hiperparametre kombinasyonlarÄ±nÄ± dener ve en iyi performans veren kombinasyonu seÃ§er. 

**NasÄ±l Ã§alÄ±ÅŸÄ±r?**
1) Ã–nce denenecek parametrelerin bir â€œÄ±zgarasÄ±â€ tanÄ±mlanÄ±r.
2) GridSearch, tÃ¼m kombinasyonlarÄ± dener
3) Her kombinasyon, genellikle cross-validation (k-katlÄ± doÄŸrulama) ile test edilir.
4) En iyi performans saÄŸlayan kombinasyon seÃ§ilir.

**Ã–rnek :**
```python
from sklearn import datasets
from sklearn.linear_model import LogisticRegression

df = datasets.load_iris()
x = df["data"]
y = df["target"]

logit = LogisticRegression(max_iter=10000)
C = [0.25, 0.5, 0.75, 1, 1.25, 1.50, 1.75, 2]

scores = []

for choice in C:
    logit.set_params(C=choice)
    logit.fit(x,y)
    scores.append(logit.score(x,y))

print(scores)
```
## 8- Kategorik Veriler
Makine Ã¶ÄŸrenmesinde kategorik veriler, sayÄ±sal olmayan ama belirli kategorileri temsil eden verilerdir. Bu veriler, modelin doÄŸrudan anlamlandÄ±ramadÄ±ÄŸÄ± metinsel veya sÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸ bilgilerdir, bu yÃ¼zden genellikle sayÄ±sal forma dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmeleri gerekir.

**a. Label Encoding (Etiketleme Kodlama)**
- Her kategoriye bir sayÄ± atanÄ±r.
- KÄ±rmÄ±zÄ± = 0 , Mavi = 1
- ğŸ“‰ Dezavantaj: Model â€œ2 > 1 > 0â€ gibi yanlÄ±ÅŸ sÄ±ralama iliÅŸkisi kurabilir.
```python
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
colors = ["red","blue","green","blue"]
encoded = encoder.fit_transform(colors)
print(encoded)

Ã§Ä±ktÄ± : 
[2 0 1 0]
```
**b. One-Hot Encoding**
- Her kategori iÃ§in ayrÄ± bir sÃ¼tun oluÅŸturur (binary 0/1 deÄŸerleriyle).
- ğŸ“ˆ Avantaj: Kategoriler arasÄ±nda sÄ±ralama iliÅŸkisi kurmaz.
- ğŸ“‰ Dezavantaj: Ã‡ok fazla kategori varsa, sÃ¼tun sayÄ±sÄ± artar (boyut patlamasÄ± yaÅŸanabilir).
```python
from sklearn.preprocessing import OneHotEncoder
import numpy as np

encoder = OneHotEncoder(sparse_output=False)

colors = np.array(["gray","yellow","black","yellow"]).reshape(-1,1)
encoded = encoder.fit_transform(colors)
print(encoded)

Ã§Ä±ktÄ± :

[[0. 1. 0.]
 [0. 0. 1.]
 [1. 0. 0.]
 [0. 0. 1.]]
```
**Ã–rnek Uygulama :** 
```python
from sklearn import linear_model
import pandas as pd

# veri Ã§ekip get_dummies ile OneHotEncoder yaptÄ±k
df = pd.read_csv("data (1).csv")
result = pd.get_dummies(df[["Car"]],dtype=int)

# Car sÃ¼tununu Volume ve Weight ile birleÅŸtirdik
x = pd.concat([df[["Volume","Weight"]],result], axis=1)
y = df["CO2"]

# modeli eÄŸittik
regr = linear_model.LinearRegression()
regr.fit(x,y)

# tahmin etmesini istediÄŸimiz arabanÄ±n sÄ±rayla Volume, Weight ve Car deÄŸerlerini girdik
# her bir araba markasÄ± sÃ¼tun olduÄŸu iÃ§in istediÄŸimiz markaya 1 verdik diÄŸerleri 0
predictedCO2 = regr.predict([[2300,1300,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]])

print(predictedCO2)

Ã§Ä±ktÄ± :
# Tahmin edilen CO2 emisyonu
[122.45153299]
```

## 9- Ã‡apraz DoÄŸrulama (Cross-Validation)
Makine Ã¶ÄŸrenmesinde Ã§apraz doÄŸrulama (cross-validation), bir modelin genelleme yeteneÄŸini yani yeni veriler Ã¼zerindeki performansÄ±nÄ± deÄŸerlendirmek iÃ§in kullanÄ±lan bir yÃ¶ntemdir.
BasitÃ§e, veriyi birkaÃ§ parÃ§aya ayÄ±rÄ±p modelin farklÄ± bÃ¶lÃ¼mler Ã¼zerinde eÄŸitilip test edilmesini saÄŸlar.

---
ğŸ”¹ **Neden Ã§apraz doÄŸrulama kullanÄ±lÄ±r?**

Bir modeli sadece tek bir train-test ayrÄ±mÄ± ile deÄŸerlendirirsek, sonuÃ§ tesadÃ¼fi bir veri bÃ¶lÃ¼nmesine baÄŸlÄ± olabilir.
Ã–rneÄŸin:

- Train verisinde model Ã§ok iyi performans gÃ¶sterir ama test verisinde kÃ¶tÃ¼ olabilir.

- Veya tam tersi, ÅŸans eseri iyi sonuÃ§ alabiliriz.

Ã‡apraz doÄŸrulama bu rastgeleliÄŸi azaltÄ±r ve daha gÃ¼venilir bir performans tahmini sunar.

---
ğŸ”¹ **En yaygÄ±n yÃ¶ntem: K-KatlÄ± Ã‡apraz DoÄŸrulama (K-Fold Cross Validation)**

**AdÄ±mlar:**

- 1- Veri kÃ¼mesi K eÅŸit parÃ§aya (foldâ€™a) bÃ¶lÃ¼nÃ¼r.

- 2- Her adÄ±mda:

  - K-1 parÃ§a eÄŸitim (train) iÃ§in,

  - 1 parÃ§a test (validation) iÃ§in kullanÄ±lÄ±r.

- 3- Bu iÅŸlem K kez tekrarlanÄ±r, her seferinde farklÄ± bir parÃ§a test verisi olur.
- 4- Her bir denemenin doÄŸruluk (accuracy, RMSE, F1, vb.) sonucu alÄ±nÄ±r.
- 5- SonuÃ§lar ortalamasÄ±, modelin genel performansÄ±nÄ± verir.
---

ğŸ”¹ **Ã–zel TÃ¼rleri**
| TÃ¼r                       | AÃ§Ä±klama                                                                                                |
| ------------------------- | ------------------------------------------------------------------------------------------------------- |
| **Stratified K-Fold**     | SÄ±nÄ±flar dengesizse, her foldâ€™da sÄ±nÄ±f oranlarÄ± korunur (Ã¶zellikle classificationâ€™da).                  |
| **Leave-One-Out (LOOCV)** | K = veri sayÄ±sÄ±. Her adÄ±mda bir gÃ¶zlem test iÃ§in ayrÄ±lÄ±r. Ã‡ok maliyetli ama kÃ¼Ã§Ã¼k verilerde kullanÄ±lÄ±r. |
| **ShuffleSplit**          | Veriyi rastgele train-test setlerine bÃ¶ler, K kez tekrarlar. Fold bÃ¼yÃ¼klÃ¼kleri sabit olmayabilir.       |
| **TimeSeriesSplit**       | Zaman serilerinde kullanÄ±lÄ±r; geÃ§miÅŸ verilerle eÄŸitilir, gelecekle test edilir. Zaman sÄ±rasÄ± korunur.   |
---

**Ã¶rnek :**
```python
from sklearn.model_selection import KFold, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from skleran.datasets import load_iris

x,y = load_iris(return_X_y=True)

model = DecisionTreeClassifier(random_state=42)

k_folds = KFold(n_splits=5)

scores = cross_val_score(model,x,y,cv=k_folds)

print(f"Cross Validation Scores: {scores}")
print(f"Average CV scores: {scores.mean()}")
print(f"Number of CV scores used in Average: {len(scores)}")
```

---
# Makinenin Ã¶ÄŸrenme biÃ§imleri
## DoÄŸrusal Regresyon

- DoÄŸrusal regresyon, bir baÄŸÄ±mlÄ± deÄŸiÅŸkeni (y) bir veya daha fazla baÄŸÄ±msÄ±z deÄŸiÅŸken (x) ile doÄŸrusal bir iliÅŸki kurarak tahmin etmeye Ã§alÄ±ÅŸÄ±r.

**Matematiksel modeli:**

>   y = ax + b 
- a (intercept): DoÄŸrunun y-eksenini kestiÄŸi nokta.

- b (slope): DoÄŸrunun eÄŸimi, yani x deÄŸiÅŸtiÄŸinde yâ€™nin nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶sterir.

Ã–rneÄŸi bir arabanÄ±n yaÅŸÄ± artarsa hÄ±zÄ± dÃ¼ÅŸer mi ? Sorusunu doÄŸrusal regresyon ile inceleyebilriz.

**Ã–rnek :**
```python
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats

x = [5,7,8,7,2,19,2,9,4,11,12,9,6]
y = [99,86,87,88,112,86,101,87,94,78,77,85,86]

a,b,r,p,std_err = stats.linregress(x,y)

def myFunc(x):
    return a*x+b

my_model = list(map(myFunc,x))
plt.scatter(x,y)
plt.plot(x,my_model)

plt.show()
```
- a = fonksiyon eÄŸimi

- b = fonksiyon sabiti
- r = korelasyon katsayÄ±sÄ±dÄ±r. Ä°ki deÄŸiÅŸken arasÄ±ndaki doÄŸrusal iliÅŸkinin gÃ¼cÃ¼nÃ¼ ve yÃ¶nÃ¼nÃ¼ gÃ¶steren bir katsayÄ±dÄ±r. DeÄŸeri -1 ile +1 arasÄ±nda deÄŸiÅŸir. 0 Ã§Ä±karsa aralarÄ±nda hiÃ§bir iliÅŸki yok deriz. EÄŸer +1 Ã§Ä±karsa aralarÄ±nda mÃ¼kemmel pozitif iliÅŸki var deriz yani ikisi de birlikte artÄ±p veya birlikte azalÄ±r anlamÄ±na gelir. -1 Ã§Ä±karsa aralarÄ±nda mÃ¼kemmel bir negatif iliÅŸki var deriz, mesela biri artarsa biri azalÄ±r.
- p = bu deÄŸer 0.05 in altÄ±nda Ã§Ä±karsa bu fonksiyon gÃ¼venilirdir yani x ile y arasÄ±nda anlamlÄ± bir iliÅŸki var anlamÄ±na gelir. ÃœstÃ¼nde Ã§Ä±karsa Ã¼rettiÄŸi deÄŸerler yanlÄ±ÅŸ Ã§Ä±kabilir.
- std_err = regresyonda bulunan eÄŸimin (a) ne kadar gÃ¼venilir olduÄŸunu gÃ¶sterir.

-   | Durum                              | Ne anlama gelir                                        |
    | ---------------------------------- | ------------------------------------------------------ |
    | `std_err` â‰ˆ 0                      | EÄŸim Ã§ok gÃ¼venilir â€” veri neredeyse mÃ¼kemmel doÄŸrusal. |
    | KÃ¼Ã§Ã¼k (`<1` veya Ã§ok kÃ¼Ã§Ã¼k)        | GÃ¼venilir model (veri Ã¶lÃ§eÄŸine baÄŸlÄ± olarak).          |
    | BÃ¼yÃ¼k (`>1` veya `>> eÄŸim deÄŸeri`) | GÃ¼rÃ¼ltÃ¼ yÃ¼ksek, eÄŸim kararsÄ±z, model zayÄ±f.            |

## Polinom Regresyonu

DoÄŸrusal regresyon (Linear Regression), veriler arasÄ±ndaki iliÅŸkiyi doÄŸru (lineer) bir denklem ile modellemeye Ã§alÄ±ÅŸÄ±r:


$$
y = b_0 + b_1 x
$$

Ancak bazÄ± veriler doÄŸrusal olmayan (non-linear) iliÅŸkilere sahiptir.
Ã–rneÄŸin, fiyat â€“ yaÅŸ, hÄ±z â€“ mesafe gibi iliÅŸkiler bir doÄŸruyla iyi aÃ§Ä±klanmaz.
Bu durumda polinom regresyon kullanÄ±lÄ±r:

$$
y = b_0 + b_1 x + b_2 x^2 + b_3 x^3 + \dots + b_n x^n
$$


Yani doÄŸrusal regresyonun Ã¼zerine xâ€™in Ã¼stlÃ¼ (kuvvetli) terimlerini ekleyerek daha esnek bir model kuruyoruz.

**Ã–rnek :**
```python
import matplotlib.pyplot as plt
import numpy as np

x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]
y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]

my_model = np.poly1d(np.polyfit(x,y,3))

my_line = np.linspace(1,22,100)

plt.scatter(x,y)
plt.plot(my_line,(my_model(my_line)))

plt.show()
```
**numpy.polyfit :** Verilere en iyi uyan polinomun katsayÄ±larÄ±nÄ± bulan bir fonksiyondur.
Yani elimizde bazÄ± noktalar (x, y) varsa, bu noktalarÄ± yakÄ±ndan geÃ§en bir polinom fonksiyon bulmaya Ã§alÄ±ÅŸÄ±r.

numpy.polyfit(x, y, deg) 
- x â†’ BaÄŸÄ±msÄ±z deÄŸiÅŸken (liste veya numpy array)
- y â†’ BaÄŸÄ±mlÄ± deÄŸiÅŸken (liste veya numpy array)
- deg â†’ Polinomun derecesi (kaÃ§Ä±ncÄ± dereceden olacaÄŸÄ±)
---
**numpy.poly1d :** polinomlarla Ã§alÄ±ÅŸmayÄ± kolaylaÅŸtÄ±ran bir sÄ±nÄ±ftÄ±r.
Elinde polinomun katsayÄ±larÄ± varsa, np.poly1d ile bunu fonksiyon gibi kullanabilirsin.

numpy.poly1d(c)
- c â†’ Polinom katsayÄ±larÄ± (liste veya array).
- KatsayÄ±lar yÃ¼ksek dereceden baÅŸlayarak verilmelidir.

> Yani np.polyfit polinomun katsayÄ±larÄ±nÄ± bulur,
np.poly1d ise bu katsayÄ±larÄ± alÄ±p polinom nesnesi yapar.

## r2_score (determinasyon katsayÄ±sÄ±)

- modelin tahminlerinin gerÃ§ek deÄŸerlere ne kadar uyduÄŸunu Ã¶lÃ§en bir metriktir.
- sana modelin veriye uyum derecesini sÃ¶yler.

**SonuÃ§:**
- 1â€™e yakÄ±nsa â†’ model Ã§ok iyi uyuyor.
- 0â€™a yakÄ±nsa â†’ model Ã§ok kÃ¶tÃ¼.
- Negatifse â†’ model, verinin ortalamasÄ±nÄ± almaktan bile kÃ¶tÃ¼.

**Ã–rnek :**
```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import r2_score

x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]
y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]

my_model = np.poly1d(np.polyfit(x,y,3))

print(r2_score(y,my_model(x)))
```
**Ã§Ä±ktÄ± :**
```python
0.9432150416451026
```
- Ã§Ä±ktÄ± 1â€™e Ã§ok yakÄ±n yani modelin tahminlerini gerÃ§ek deÄŸerlere Ã§ok iyi uyuyor.

## Ã‡oklu Regresyon (Multiple Regression)

Ã‡oklu regresyon (multiple regression), bir baÄŸÄ±mlÄ± deÄŸiÅŸkeni (yani sonucu) **birden fazla baÄŸÄ±msÄ±z deÄŸiÅŸken kullanarak tahmin etmek** iÃ§in kullanÄ±lan istatistiksel bir yÃ¶ntemdir.

- BaÄŸÄ±mlÄ± deÄŸiÅŸken (Y): Tahmin etmek istediÄŸimiz deÄŸiÅŸken.
Ã–rnek: Ev fiyatÄ±.

- BaÄŸÄ±msÄ±z deÄŸiÅŸkenler (X1, X2, X3â€¦): BaÄŸÄ±mlÄ± deÄŸiÅŸkeni etkileyebilecek faktÃ¶rler.
Ã–rnek: Ev bÃ¼yÃ¼klÃ¼ÄŸÃ¼, oda sayÄ±sÄ±, semt, yaÅŸ gibi.

**Ã‡oklu regresyon denklemi ÅŸÃ¶yle yazÄ±lÄ±r:**
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n + \varepsilon
$$

**AÃ§Ä±klamalar:**

- $Y$ = BaÄŸÄ±mlÄ± deÄŸiÅŸken
- $X_1, X_2, \dots, X_n$ = BaÄŸÄ±msÄ±z deÄŸiÅŸkenler
- $\beta_0$ = Y kesiti (intercept)
- $\beta_1, \beta_2, \dots, \beta_n$ = BaÄŸÄ±msÄ±z deÄŸiÅŸkenlerin katsayÄ±larÄ±
- $\varepsilon$ = Hata terimi (modelin tahmin edemediÄŸi kÄ±smÄ±)

**Ã–rnek :**
```python
from sklearn import linear_model
import pandas as pd

# dosya okuma
df = pd.read_csv("data (1).csv")

# deÄŸer atama
x = df[["Volume","Weight"]]
y = df["CO2"]

# makine eÄŸitimi
regr = linear_model.LinearRegression()
regr.fit(x,y)

# kendi deÄŸerini verip tahmin yaptÄ±rma
new_data = pd.DataFrame([[3300,1300]],columns=["Volume","Weight"])
result = regr.predict(new_data)
print(result)
```
## Karar aÄŸacÄ± (Decision Tree)
SÄ±nÄ±flandÄ±rma ve regresyon problemlerinde kullanÄ±lan denetimli bir makine Ã¶ÄŸrenmesi algoritmasÄ±dÄ±r. Veri setini, Ã¶zelliklerin deÄŸerlerine gÃ¶re dallara ayÄ±rarak karar kurallarÄ± Ã¼retir. SonuÃ§ta kÃ¶kten yapraklara doÄŸru ilerleyerek tahmin yapÄ±lÄ±r.

**Karar AÄŸacÄ± MantÄ±ÄŸÄ±**

- KÃ¶k dÃ¼ÄŸÃ¼m (root node): Verinin ilk bÃ¶lÃ¼ndÃ¼ÄŸÃ¼ yerdir.

- Dallar (branches): Kararlara gÃ¶re verinin ayrÄ±ldÄ±ÄŸÄ± yollardÄ±r.

- Yaprak dÃ¼ÄŸÃ¼mler (leaf nodes): Nihai karar veya sÄ±nÄ±flandÄ±rmadÄ±r.

Bir soru gibi Ã§alÄ±ÅŸÄ±r :

ğŸ‘‰ â€œÃ–zellik A ÅŸu deÄŸerden kÃ¼Ã§Ã¼k mÃ¼?â€ â†’ Evetse sol dal, hayÄ±rsa saÄŸ dal.
Bu sÃ¼reÃ§ devam eder, en sonunda sÄ±nÄ±flandÄ±rma ya da tahmin yapÄ±lÄ±r.

**Ã–rnek :**

- Bu kodda kullanÄ±lan datada bir komedyenin Ã¶zelliklerine gÃ¶re onun gÃ¶sterisine gidip gitmediÄŸimiz var. W3School'un decision tree bÃ¶lÃ¼mÃ¼nden. 
```python
from sklearn.tree import DecisionTreeClassifier
import pandas as pd

df = pd.read_csv("decision_tree.csv")

# datadaki string verileri sayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz (Makinenin anlayacaÄŸÄ± dilden konuÅŸuyoruz).
dN = {"UK":0,"USA":1,"TR":2}
df["Nationality"] = df["Nationality"].map(dN)
dGo = {"YES":1,"NO":0}
df["Go"] = df["Go"].map(dGo)

# x ve y deÄŸerlerini tanÄ±mlayalÄ±m
X = df[["Age","Experience","Rank","Nationality"]]
y = df["Go"]

# makinenin Ã¶ÄŸrenme tipini seÃ§elim ve Ã¶ÄŸretelim
dtree = DecisionTreeClassifier()
dtree.fit(X,y)

# kendimiz deÄŸer verip makineden tahmin yapmasÄ±nÄ± isteyelim
new_data = pd.DataFrame([[40,10,7,1]],columns=["Age","Experience","Rank","Nationality"])
result = dtree.predict(new_data)
print(result)
```
> Bu kod Ã§Ä±ktÄ± olarak [0] ya da [1] Ã¼retecek yani gitmem veya giderim.

## HiyerarÅŸik kÃ¼meleme (Hierarchical Clustering)
Verileri kÃ¼melere ayÄ±rmak iÃ§in kullanÄ±lan bir kÃ¼meleme (clustering) yÃ¶ntemidir. Bu yÃ¶ntemde amaÃ§, benzer Ã¶zelliklere sahip verileri aÄŸaÃ§ benzeri bir yapÄ± (dendrogram) ÅŸeklinde gruplayarak aralarÄ±ndaki iliÅŸkileri gÃ¶stermektir.

- HiyerarÅŸik kÃ¼meleme, gÃ¶zlemler arasÄ±ndaki benzerlikleri (Ã¶rneÄŸin mesafeleri) kullanarak bir kÃ¼me yapÄ±sÄ± (hiyerarÅŸi) oluÅŸturur.
- SonuÃ§ta, hangi verilerin birbirine daha yakÄ±n olduÄŸunu gÃ¶steren bir aÄŸaÃ§ (dendrogram) elde ederiz.

**Bu aÄŸaÃ§ sayesinde :**
- KaÃ§ kÃ¼me olacaÄŸÄ±na sen karar verebilirsin (aÄŸacÄ± belli bir yÃ¼kseklikten keserek).
- KÃ¼meleme iliÅŸkilerini gÃ¶rsel olarak inceleyebilirsin.

ğŸ§­ **Ä°ki tÃ¼rÃ¼ vardÄ±r :**

---
- **a) BirleÅŸtirici (Agglomerative)** â€“ En Ã§ok kullanÄ±lan

  - En kÃ¼Ã§Ã¼k birimden (tek tek noktalardan) baÅŸlar.
  - En yakÄ±n iki noktayÄ± birleÅŸtirir â†’ sonra bu yeni kÃ¼meyi diÄŸerlerine gÃ¶re yeniden deÄŸerlendirir.
  - Bu iÅŸlem, tÃ¼m noktalar tek bir kÃ¼mede toplanana kadar sÃ¼rer.

- **b) BÃ¶lÃ¼cÃ¼ (Divisive)**

  - TÃ¼m veriler tek bir kÃ¼me olarak baÅŸlar.
  - En uzak noktalarÄ± birbirinden ayÄ±rarak kÃ¼meleri bÃ¶lmeye baÅŸlar.
  - Sonunda her nokta kendi baÅŸÄ±na kalÄ±r.

ğŸ§® **Benzerlik NasÄ±l Ã–lÃ§Ã¼lÃ¼r?**

---
Veriler arasÄ±ndaki uzaklÄ±k (distance) genellikle ÅŸu yÃ¶ntemle Ã¶lÃ§Ã¼lÃ¼r:

- **Ã–klid mesafesi (Euclidean distance)**
  - Ã–klid mesafesi, iki nokta arasÄ±ndaki doÄŸrusal uzaklÄ±ÄŸÄ± Ã¶lÃ§er.
Yani dÃ¼z bir Ã§izgiyle bir noktadan diÄŸerine olan mesafedir.
Matematikte Pisagor teoremine dayanÄ±r.

**FormÃ¼lÃ¼ :**

Ä°ki nokta olsun:
A(xâ‚, yâ‚) ve B(xâ‚‚, yâ‚‚)
$$
d(A, B) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
$$

ğŸ”— **KÃ¼meler ArasÄ±ndaki UzaklÄ±k (BaÄŸlantÄ± / Linkage)**

---
Ä°ki kÃ¼me birleÅŸtirilirken aralarÄ±ndaki mesafenin nasÄ±l Ã¶lÃ§Ã¼leceÄŸini belirleyen baÄŸlantÄ± yÃ¶ntemi seÃ§ilir:

- **Wardâ€™s Method**
  - KÃ¼meler birleÅŸince varyans artÄ±ÅŸÄ± en az olacak ÅŸekilde birleÅŸtirir
  - Genellikle en baÅŸarÄ±lÄ± yÃ¶ntem
  
**FormÃ¼lÃ¼:**
$$
\Delta E_{AB} = \frac{n_A \cdot n_B}{n_A + n_B} \times \|\bar{x}_A - \bar{x}_B\|^2
$$

- **nA :** KÃ¼me Aâ€™daki gÃ¶zlem (veri noktasÄ±) sayÄ±sÄ±  
- **nB :** KÃ¼me Bâ€™deki gÃ¶zlem (veri noktasÄ±) sayÄ±sÄ±  
- **xÌ„A :** KÃ¼me Aâ€™nÄ±n ortalama (merkez) vektÃ¶rÃ¼  
- **xÌ„B :** KÃ¼me Bâ€™nin ortalama (merkez) vektÃ¶rÃ¼  
- **||xÌ„A âˆ’ xÌ„B|| :** Ä°ki kÃ¼menin merkezleri arasÄ±ndaki Ã–klid mesafesi  
- **Î”EAB :** Ä°ki kÃ¼me birleÅŸtirildiÄŸinde toplam varyanstaki artÄ±ÅŸ miktarÄ±

**Ã–rnek :**
- **HiyerarÅŸik kÃ¼meleme grafiÄŸi :**
```python
from scipy.cluster.hierarchy import dendrogram,linkage
import matplotlib.pyplot as plt
import numpy as np

x = np.array([4,5,10,4,3,11,14,6,10,12])
y = np.array([21,19,24,17,16,25,24,22,21,21])

data = list(zip(x,y))
l_data = linkage(data,method="ward",metric="euclidean")
dendrogram(l_data)
plt.show()
```
- **AgglomerativeClustering kullanÄ±mÄ± :**
```python
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
import numpy as np

x = np.array([4,5,10,4,3,11,14,6,10,12])
y = np.array([21,19,24,17,16,25,24,22,21,21])

data = list(zip(x,y))
l_data = AgglomerativeClustering(n_clusters=2,linkage="ward")
labels = l_data.fit_predict(data)

plt.scatter(x,y,c=labels)
plt.show()
```

## Lojistik regresyon
Lojistik regresyon, bir olayÄ±n olma olasÄ±lÄ±ÄŸÄ±nÄ± tahmin eder.
Ã‡Ä±ktÄ± sÃ¼rekli deÄŸil, 0 veya 1 (Ã¶rneÄŸin: hasta / saÄŸlÄ±klÄ±, evet / hayÄ±r) gibi sÄ±nÄ±flar olur.

**Temel FormÃ¼l :**
$$
P(y=1 \mid x) = \frac{1}{1 + e^{-(b_0 + b_1 x_1 + b_2 x_2 + \dots + b_n x_n)}}
$$
- Bu formÃ¼l, elimizdeki bir girdiye (Ã¶zelliklere, x) bakarak, olayÄ±n gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±nÄ± (y = 1) hesaplar.
- â€œBu Ã¶zelliklere sahip bir Ã¶rnek, 1 sÄ±nÄ±fÄ±na (Ã¶rneÄŸin â€˜hastaâ€™, â€˜geÃ§tiâ€™, â€˜evetâ€™) ait olma olasÄ±lÄ±ÄŸÄ± nedir?â€
sorusunun cevabÄ±nÄ± verir.

**Ã–rnek :** 
- FormÃ¼lÃ¼ kullanarak kanser olma olasÄ±lÄ±klarÄ±nÄ± hesapladÄ±k.
```python
from sklearn import linear_model
import numpy as np

X = np.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)
y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])

logr = linear_model.LogisticRegression()
logr.fit(X,y)

def logitP(logr,X):
  log_odds = logr.coef_ * X + logr.intercept_
  odds = np.exp(log_odds)
  probability = odds / (1 + odds)
  return probability

print(logitP(logr,X))
```
**Ã‡Ä±ktÄ± :**
```python
[[0.60749168]
 [0.19267555]
 [0.12774788]
 [0.00955056]
 [0.08037781]
 [0.0734485 ]
 [0.88362857]
 [0.77901203]
 [0.88924534]
 [0.81293431]
 [0.57718238]
 [0.96664398]]
```
- Yine aynÄ± fomÃ¼lÃ¼ kullandÄ±k ama bu sefer formÃ¼lÃ¼ kendimiz yazmadÄ±k. **predict_proba** modÃ¼lÃ¼yle yaptÄ±k.
```python
from sklearn import linear_model
import numpy as np

X = np.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)
y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])

# Ã–ÄŸrenme
logr = linear_model.LogisticRegression()
logr.fit(X,y)

# kanser olma olasÄ±lÄ±ÄŸÄ± hesaplanacak hastanÄ±n tÃ¼mor bÃ¼yÃ¼klÃ¼ÄŸÃ¼
yeni_hasta = np.array([[3.78]])

# tahmin
result = logr.predict_proba(yeni_hasta)[0,1]
print(result)
```
**Ã‡Ä±ktÄ± :**
```python
0.6074916769842938
```

## K-Means
makine Ã¶ÄŸrenmesinde kullanÄ±lan en popÃ¼ler kÃ¼meleme (clustering) yÃ¶ntemlerinden biridir. GÃ¶zetimsiz (unsupervised) Ã¶ÄŸrenme kategorisine girer; yani verilerin etiketleri (Ã¶rneÄŸin sÄ±nÄ±f bilgisi) yoktur.

- K-Means, veriyi K tane kÃ¼meye (gruba) ayÄ±ran bir algoritmadÄ±r.
- AmaÃ§, her kÃ¼me iÃ§indeki verilerin birbirine benzer, farklÄ± kÃ¼melerdeki verilerin ise farklÄ± olmasÄ±nÄ± saÄŸlamaktÄ±r.

**Dirsek (Elbow) yÃ¶ntemi ile K deÄŸeri bulma (n_clusters) :**
```python
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

x = [4,5,10,4,3,11,14,6,10,12]
y = [21,19,24,17,16,25,24,22,21,21]

data = list(zip(x,y))
inertias = []

for i in range(1,11):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(data)
    inertias.append(kmeans.inertia_)

plt.plot(range(1,11),inertias,marker="o")
plt.title("Elbow method")
plt.xlabel("Number of clusters")
plt.ylabel("Inertia")
plt.show()
```
> Bulunan K deÄŸeri : 2

**Bulunan K deÄŸerini uygulama :**
```python
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

x = [4,5,10,4,3,11,14,6,10,12]
y = [21,19,24,17,16,25,24,22,21,21]

data = list(zip(x,y))

kmeans = KMeans(n_clusters=2)
kmeans.fit(data)

plt.scatter(x,y,c=kmeans.label_)
plt.show()
```

## K En yakÄ±n KomÅŸu (K-nearest neighbors)
Makine Ã¶ÄŸrenmesinde K En YakÄ±n KomÅŸu (K-Nearest Neighbors, KNN) algoritmasÄ±, denetimli (supervised) Ã¶ÄŸrenme yÃ¶ntemlerinden biridir ve hem sÄ±nÄ±flandÄ±rma (classification) hem de regresyon (regression) problemlerinde kullanÄ±labilir.

**KNNâ€™nin temel mantÄ±ÄŸÄ± ÅŸudur :**

- â€œBir Ã¶rneÄŸin sÄ±nÄ±fÄ±, ona en yakÄ±n komÅŸularÄ±nÄ±n Ã§oÄŸunlukta olduÄŸu sÄ±nÄ±fa aittir.â€

Yani, yeni bir verinin etiketini tahmin etmek istediÄŸimizde:

- **1-** EÄŸitim verisindeki tÃ¼m noktalarla arasÄ±ndaki mesafeyi hesaplarÄ±z.

- **2-** Bu mesafelere gÃ¶re en yakÄ±n K komÅŸuyu seÃ§eriz.

- **3-** Bu K komÅŸunun etiketlerine bakarak tahmin yaparÄ±z :
  - **SÄ±nÄ±flandÄ±rma** iÃ§in: KomÅŸular arasÄ±nda en Ã§ok gÃ¶rÃ¼len sÄ±nÄ±f seÃ§ilir (Ã§oÄŸunluk oylamasÄ±).

  - **Regresyon** iÃ§in: KomÅŸularÄ±n ortalamasÄ± alÄ±nÄ±r.

ğŸ”¢ **AdÄ±m AdÄ±m KNN AlgoritmasÄ± :**

Ã–rneÄŸin sÄ±nÄ±flandÄ±rma iÃ§in :

- **1-** K deÄŸeri belirlenir (Ã¶rneÄŸin K = 3).
- **2-** Test verisi ile tÃ¼m eÄŸitim verisi arasÄ±ndaki mesafe hesaplanÄ±r.(Ã–klid ile)
- **3-** En kÃ¼Ã§Ã¼k mesafeye sahip K komÅŸu seÃ§ilir.
- **4-** Bu komÅŸularÄ±n etiketleri sayÄ±lÄ±r.
- **5-** En sÄ±k gÃ¶rÃ¼len sÄ±nÄ±f etiketi yeni veriye atanÄ±r.

âš™ï¸ **Ã–nemli Parametreler ve Dikkat Edilmesi Gerekenler**

| Konu                        | AÃ§Ä±klama                                                                                                                                                            |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **K deÄŸeri**                | KÃ¼Ã§Ã¼k K â†’ gÃ¼rÃ¼ltÃ¼ye duyarlÄ±, aÅŸÄ±rÄ± Ã¶ÄŸrenme riski.<br>BÃ¼yÃ¼k K â†’ daha dÃ¼zgÃ¼n karar sÄ±nÄ±rlarÄ± ama daha az esneklik. Genelde K, âˆšn civarÄ±nda seÃ§ilir (n = veri sayÄ±sÄ±). |
| **Mesafe metriÄŸi**          | Ã–klid, Manhattan, Minkowski, KosinÃ¼s benzerliÄŸi vs.                                                                                                                 |
| **Ã–zellik Ã¶lÃ§eklendirmesi** | KNNâ€™de mesafeler Ã¶nemli olduÄŸu iÃ§in, Ã¶zellikler aynÄ± Ã¶lÃ§ekte olmalÄ± (**standardizasyon veya normalizasyon** yapÄ±lmalÄ±).                                             |
| **AÄŸÄ±rlÄ±klÄ± oylama**        | BazÄ± varyantlarda, yakÄ±n komÅŸulara daha fazla aÄŸÄ±rlÄ±k verilir (Ã¶rneÄŸin mesafeyle ters orantÄ±lÄ± olarak).                                                             |

**Ã–rnek :**
```python
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt

x = [4,5,10,4,3,11,14,8,10,12]
y = [21,19,24,17,16,25,24,22,21,21]

classes = [0,0,1,0,0,1,1,0,1,1]

data = list(zip(x,y))

model = KNeighborsClassifier(n_neighbors=1)
model.fit(data,classes)

new_x = 8
new_y = 21
new_point = [(new_x,new_y)]

prediction = model.predict(new_point)
print(prediction) # [0] mÄ± [1] mi diye tahmin eder

#tahminin data ile birlikte gÃ¶rselleÅŸtirilmesi
plt.scatter(x+[new_x],y+[new_y],c=classes+[prediction[0]])
plt.show()
```